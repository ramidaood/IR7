{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d7663c",
   "metadata": {},
   "source": [
    "## Option 2 — Search over mixed audio/text using noisy ASR (phoneme 3-grams)\n",
    "\n",
    "This notebook implements: **g2p_en → phoneme 3-grams → cosine similarity → ranking + bar chart + top overlap**.\n",
    "\n",
    "- Query is **clean** and has **exactly 4 words**.\n",
    "- Two docs are clean text; two are ASR-like noisy with controlled corruption of **query words only**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2725c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ramidaood/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/ramidaood/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"Option 2 (IR): Phoneme 3-gram indexing and retrieval over mixed clean/noisy (ASR-like) text.\n",
    "\n",
    "Pipeline (kept as required): g2p_en → phoneme 3-grams → cosine similarity → ranking + bar chart + top overlap.\n",
    "\n",
    "Requirements (in your environment):\n",
    "  pip install g2p_en matplotlib nltk\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "try:\n",
    "    from g2p_en import G2p\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"ERROR: Could not import g2p_en. Install dependencies:\\n\"\n",
    "        \"  pip install g2p_en nltk matplotlib\\n\\n\"\n",
    "        f\"Details: {e}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    raise\n",
    "\n",
    "\n",
    "_PUNCT_RE = re.compile(r\"[^a-z0-9\\s]+\", re.IGNORECASE)\n",
    "_WS_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Lowercase and strip punctuation, then collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = _PUNCT_RE.sub(\" \", text)\n",
    "    text = _WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def _ensure_nltk_data() -> None:\n",
    "    \"\"\"Try to download common NLTK resources used by g2p_en if missing (quietly).\"\"\"\n",
    "    try:\n",
    "        import nltk  # type: ignore\n",
    "\n",
    "        # g2p_en relies on tokenization + POS tagging; recent NLTK versions\n",
    "        # may require the *_eng tagger variant. Keep this best-effort and quiet.\n",
    "        pkgs = (\n",
    "            \"cmudict\",\n",
    "            \"punkt\",\n",
    "            \"punkt_tab\",  # some installs need this\n",
    "            \"averaged_perceptron_tagger\",\n",
    "            \"averaged_perceptron_tagger_eng\",\n",
    "        )\n",
    "\n",
    "        for pkg in pkgs:\n",
    "            try:\n",
    "                nltk.data.find(f\"corpora/{pkg}\")\n",
    "                continue\n",
    "            except LookupError:\n",
    "                pass\n",
    "            try:\n",
    "                nltk.data.find(f\"tokenizers/{pkg}\")\n",
    "                continue\n",
    "            except LookupError:\n",
    "                pass\n",
    "            try:\n",
    "                nltk.data.find(f\"taggers/{pkg}\")\n",
    "                continue\n",
    "            except LookupError:\n",
    "                pass\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception:\n",
    "                # Best-effort only.\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def text_to_phonemes(text: str, g2p: G2p) -> List[str]:\n",
    "    \"\"\"Normalize text, then convert to a filtered phoneme token sequence (ARPAbet-like).\"\"\"\n",
    "    norm = normalize_text(text)\n",
    "    if not norm:\n",
    "        return []\n",
    "\n",
    "    raw = g2p(norm)\n",
    "\n",
    "    phonemes: List[str] = []\n",
    "    for tok in raw:\n",
    "        if not tok or tok.isspace():\n",
    "            continue\n",
    "        # Keep ARPAbet-ish tokens, e.g. AH0, N, SH.\n",
    "        if re.fullmatch(r\"[A-Z]+[0-2]?\", tok):\n",
    "            phonemes.append(tok)\n",
    "    return phonemes\n",
    "\n",
    "\n",
    "def phoneme_ngrams(phonemes: List[str], n: int = 3) -> List[str]:\n",
    "    \"\"\"Generate phoneme n-grams (default: 3-grams) as 'PH1-PH2-PH3' strings.\"\"\"\n",
    "    if n <= 0 or len(phonemes) < n:\n",
    "        return []\n",
    "    return [\"-\".join(phonemes[i : i + n]) for i in range(len(phonemes) - n + 1)]\n",
    "\n",
    "\n",
    "def vectorize(ngrams: Iterable[str]) -> Counter:\n",
    "    \"\"\"Convert an iterable of n-grams into a bag-of-ngrams count vector.\"\"\"\n",
    "    return Counter(ngrams)\n",
    "\n",
    "\n",
    "def cosine_similarity(v1: Counter, v2: Counter) -> float:\n",
    "    \"\"\"Cosine similarity for sparse count vectors; safe for empty vectors.\"\"\"\n",
    "    if not v1 or not v2:\n",
    "        return 0.0\n",
    "\n",
    "    dot = 0.0\n",
    "    # Iterate the smaller vector.\n",
    "    if len(v1) > len(v2):\n",
    "        v1, v2 = v2, v1\n",
    "    for k, c in v1.items():\n",
    "        dot += c * v2.get(k, 0)\n",
    "\n",
    "    n1 = math.sqrt(sum(c * c for c in v1.values()))\n",
    "    n2 = math.sqrt(sum(c * c for c in v2.values()))\n",
    "    if n1 == 0.0 or n2 == 0.0:\n",
    "        return 0.0\n",
    "    return float(dot / (n1 * n2))\n",
    "\n",
    "\n",
    "def _snippet(text: str, n: int = 80) -> str:\n",
    "    t = text[:n]\n",
    "    return t + (\"...\" if len(text) > n else \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af101f3b",
   "metadata": {},
   "source": [
    "## Run: build documents (clean + ASR-noisy), index with phoneme 3-grams, and retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549382b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (clean): signal processing noise reduction\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramidaood/UNI/IR7/.venv/lib/python3.13/site-packages/g2p_en/g2p.py:156: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  text = re.sub(\"[^ a-z'.,?!\\-]\", \"\", text)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - '/Users/ramidaood/nltk_data'\n    - '/Users/ramidaood/UNI/IR7/.venv/nltk_data'\n    - '/Users/ramidaood/UNI/IR7/.venv/share/nltk_data'\n    - '/Users/ramidaood/UNI/IR7/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    106\u001b[39m     plt.show()\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuery (clean):\u001b[39m\u001b[33m\"\u001b[39m, query)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m q_ph = \u001b[43mtext_to_phonemes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg2p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m q_3g = phoneme_ngrams(q_ph, n=\u001b[32m3\u001b[39m)\n\u001b[32m     46\u001b[39m q_vec = vectorize(q_3g)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtext_to_phonemes\u001b[39m\u001b[34m(text, g2p)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m norm:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m raw = \u001b[43mg2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m phonemes: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m raw:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UNI/IR7/.venv/lib/python3.13/site-packages/g2p_en/g2p.py:162\u001b[39m, in \u001b[36mG2p.__call__\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# tokenization\u001b[39;00m\n\u001b[32m    161\u001b[39m words = word_tokenize(text)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m tokens = \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# tuples of (word, tag)\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# steps\u001b[39;00m\n\u001b[32m    165\u001b[39m prons = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UNI/IR7/.venv/lib/python3.13/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UNI/IR7/.venv/lib/python3.13/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UNI/IR7/.venv/lib/python3.13/site-packages/nltk/tag/perceptron.py:180\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang, loc)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.save_dir = path_join(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.TRAINED_TAGGER_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.TAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UNI/IR7/.venv/lib/python3.13/site-packages/nltk/tag/perceptron.py:277\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang, loc)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, loc=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_param\u001b[39m(json_file):\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UNI/IR7/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - '/Users/ramidaood/nltk_data'\n    - '/Users/ramidaood/UNI/IR7/.venv/nltk_data'\n    - '/Users/ramidaood/UNI/IR7/.venv/share/nltk_data'\n    - '/Users/ramidaood/UNI/IR7/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # Clean query (exactly 4 words; no errors in query)\n",
    "    query = \"signal processing noise reduction\"\n",
    "    assert len(query.split()) == 4, \"Assignment requirement: query must contain exactly 4 words\"\n",
    "\n",
    "    # Four documents in-code: 2 clean, 2 ASR-like noisy.\n",
    "    # ASR errors are injected ONLY in query words, with controlled diversity:\n",
    "    # - D3_noisy_asr corrupts ONLY TWO query words; keeps the other two correct.\n",
    "    # - D4_noisy_asr corrupts the OTHER TWO query words; keeps the first two correct.\n",
    "    documents = [\n",
    "        Document(\n",
    "            \"D1_clean\",\n",
    "            \"This assignment discusses signal processing for audio, including noise reduction \"\n",
    "            \"methods and evaluation of retrieval quality.\",\n",
    "        ),\n",
    "        Document(\n",
    "            \"D2_clean\",\n",
    "            \"We study information retrieval over spoken content. Signal processing can help \"\n",
    "            \"with noise reduction before indexing.\",\n",
    "        ),\n",
    "        # D3 corrupts: signal→signel, noise→noize; keeps: processing, reduction\n",
    "        Document(\n",
    "            \"D3_noisy_asr\",\n",
    "            \"This assignment discusses signel processing for audio, including noize reduction \"\n",
    "            \"methods and evaluation of retrieval quality.\",\n",
    "        ),\n",
    "        # D4 corrupts: processing→proccessing, reduction→reduccion; keeps: signal, noise\n",
    "        Document(\n",
    "            \"D4_noisy_asr\",\n",
    "            \"We study information retrieval over spoken content. signal proccessing can help \"\n",
    "            \"with noise reduccion before indexing.\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    _ensure_nltk_data()\n",
    "    g2p = G2p()\n",
    "\n",
    "    print(\"Query (clean):\", query)\n",
    "    print()\n",
    "\n",
    "    q_ph = text_to_phonemes(query, g2p)\n",
    "    q_3g = phoneme_ngrams(q_ph, n=3)\n",
    "    q_vec = vectorize(q_3g)\n",
    "\n",
    "    doc_vectors: Dict[str, Counter] = {}\n",
    "    doc_scores: List[Tuple[str, float]] = []\n",
    "\n",
    "    print(\"Documents (normalized snippets):\")\n",
    "    for d in documents:\n",
    "        norm = normalize_text(d.text)\n",
    "        print(f\"- {d.doc_id}: {_snippet(norm)}\")\n",
    "\n",
    "        d_ph = text_to_phonemes(d.text, g2p)\n",
    "        d_3g = phoneme_ngrams(d_ph, n=3)\n",
    "        d_vec = vectorize(d_3g)\n",
    "\n",
    "        doc_vectors[d.doc_id] = d_vec\n",
    "        score = cosine_similarity(q_vec, d_vec)\n",
    "        doc_scores.append((d.doc_id, score))\n",
    "    print()\n",
    "\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Results (ranked by cosine similarity over phoneme 3-grams):\")\n",
    "    print(f\"{'doc_id':<12} {'score':>10}\")\n",
    "    print(\"-\" * 24)\n",
    "    for doc_id, score in doc_scores:\n",
    "        print(f\"{doc_id:<12} {score:>10.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Top overlap section for best-ranked document.\n",
    "    best_doc_id, best_score = doc_scores[0]\n",
    "    best_vec = doc_vectors[best_doc_id]\n",
    "\n",
    "    overlap = {k: min(q_vec.get(k, 0), best_vec.get(k, 0)) for k in (q_vec.keys() & best_vec.keys())}\n",
    "    top_overlap = sorted(\n",
    "        overlap.items(),\n",
    "        key=lambda kv: (kv[1], q_vec[kv[0]] + best_vec[kv[0]]),\n",
    "        reverse=True,\n",
    "    )[:15]\n",
    "\n",
    "    print(f\"Top overlap (best doc: {best_doc_id}, score={best_score:.4f})\")\n",
    "    if not top_overlap:\n",
    "        print(\"- (no overlapping 3-grams)\")\n",
    "    else:\n",
    "        print(f\"{'3-gram':<20} {'q':>4} {'doc':>5} {'overlap':>8}\")\n",
    "        print(\"-\" * 42)\n",
    "        for gram, ov in top_overlap:\n",
    "            print(f\"{gram:<20} {q_vec[gram]:>4} {best_vec[gram]:>5} {ov:>8}\")\n",
    "    print()\n",
    "\n",
    "    # Visualization: bar chart of cosine similarity per doc.\n",
    "    doc_ids = [doc_id for doc_id, _ in doc_scores]\n",
    "    scores = [score for _, score in doc_scores]\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.bar(doc_ids, scores)\n",
    "    plt.title(\"Cosine similarity (query vs documents) using phoneme 3-grams\")\n",
    "    plt.xlabel(\"Document\")\n",
    "    plt.ylabel(\"Cosine similarity\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
